// Copyright 2018 Google Inc. All rights reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#pragma kernel RayTrace
#pragma kernel InitCameraRays
#pragma kernel NormalizeSamples

// ---------------------------------------------------------------------------------------------- //
// Data Structures, Sampling, and Shading.
// ---------------------------------------------------------------------------------------------- //

#include "Structures.cginc"
#include "Sampling.cginc"
#include "Shading.cginc"

// ---------------------------------------------------------------------------------------------- //
// Kernel Inputs & Outputs.
// ---------------------------------------------------------------------------------------------- //

int _MaxBounces;

float4x4 _Camera;
float4x4 _CameraI;
float4x4 _Projection;
float4x4 _ProjectionI;

float _Aperture;
float _FocusDistance;
int _ActiveMaterial;

Texture2D<float4> _AccumulatedImage;
StructuredBuffer<Sphere> _Spheres;
RWStructuredBuffer<Ray> _Rays;

// ---------------------------------------------------------------------------------------------- //
// Scene Intersection
// ---------------------------------------------------------------------------------------------- //

bool HitSpheres(Ray r, float tMin, float tMax, out HitRecord rec) {
  rec.t = -1.0;
  rec.p = vec3(0,0,0);
  rec.normal = vec3(0,0,0);
  rec.uv = vec2(0,0);
  rec.albedo = vec3(1,0,0);
  rec.material = 0;

  HitRecord tempRec;
  bool hitAnything = false;
  float closestSoFar = tMax;
  uint numStructs;
  uint stride;
  _Spheres.GetDimensions(numStructs, stride);

  //
  // GPU NOTE: A long running for-loop is often the wrong/inefficient choice for a compute shader.
  //           Typically it's better to reformulate the problem in terms of compute threads.
  //
  for (uint i = 0; i < numStructs; i++) {
    if (_Spheres[i].Hit(r, tMin, closestSoFar, tempRec)) {
      hitAnything = true;
      closestSoFar = tempRec.t;
      rec = tempRec;
    }
  }

  return hitAnything;
}

// ---------------------------------------------------------------------------------------------- //
// Main Shading Entry Point.
// ---------------------------------------------------------------------------------------------- //

bool Color(inout Ray r, vec2 uv) {
  HitRecord rec;
  rec.uv = vec2(0,0);
  rec.t = -1.0;
  rec.p = vec3(0,0,0);
  rec.normal = vec3(1,0,0);
  rec.albedo = vec3(1,0,0);
  rec.material = 0;

  r.bounces++;

  if (HitSpheres(r, 0.001, FLT_MAX, rec)) {
    rec.uv = uv;
    Ray scattered = r;
    vec3 attenuation = vec3(0,0,0);

    // Performance note: Here the cost of all shading functions is paid for every ray. We can reduce
    // divergence and improve utilization by shading after tracing rays, though as with traditional
    // deferred shading, doing so will require more memory and bandwidth.

    if (ScatterLambertian(r, rec, attenuation, scattered)
        || ScatterMetal(r, rec, attenuation, scattered)
        || ScatterDielectric(r, rec, attenuation, scattered)) {
      scattered.color = r.color * attenuation;
      r = scattered;
      return true;
    } else {
      return false;
    }
  } else {
    r.color *= EnvColor(r, uv);
    return false;
  }
}

// ---------------------------------------------------------------------------------------------- //
// Get Next Ray
//
// Increment the counter built into the _Rays buffer to select the next ray. The rayIndex dictates
// the location in the _Rays buffer, as well as the location in screen space. Rays are supersampled
// in parallel and the array layout is shown in the diagram below.
//
// _Rays array layout for a 3x2 image with 3x super sampling:
// 
// *--*--*--*  *--*--*--*  *--*--*--*
// | 0| 2| 4|  | 6| 8|10|  |12|14|16|
// *--*--*--*  *--*--*--*  *--*--*--*
// | 1| 3| 5|  | 7| 9|11|  |13|15|17|
// *--*--*--*  *--*--*--*  *--*--*--*
//
// The default buffer is 1024 x 512 with 16x super sampling.
// ---------------------------------------------------------------------------------------------- //

void GetNextRay(uint3 id, uint rayCount, uint width, uint height,
            out int2 xy, out vec2 uv, out int rayIndex, out Ray ray) {
  //
  // The array counter points to the last processed ray and is incremented to process the next ray.
  //
  rayIndex = _Rays.IncrementCounter() % rayCount;
  //rayIndex = ((id.x * height + id.y) + (width * height * id.z)) % rayCount;

  ray = _Rays[rayIndex];

  //
  // Convert the ray index into 2D pixel screen coordinates.
  //
  xy = int2(rayIndex / height % width, rayIndex % height);

  //
  // Screen space UV in [0, 1].
  //
  uv = vec2(xy.x  / (float)width, 1 - xy.y / (float)height);

  //
  // Pixels are super sampled, so jitter by random (0, dx) (0, dy).
  //
  uv += Noise(uv + _R0 * id.z) * vec2(1/(float)width, 1/(float)height);
}

// ---------------------------------------------------------------------------------------------- //
// Camera Integration for Primary Rays.
//
// Strictly speaking, this is just initializing primary rays from inverse View & Projection
// matricies and is not Unity specific.
// ---------------------------------------------------------------------------------------------- //

Ray InitCameraRay(vec2 uv) {
  //
  // Compute the NDC-space (0,1) focus distance by projecting from camera-space.
  //
  vec4 focalPlane = vec4(0, 0, -_FocusDistance * 2, 1);
  focalPlane = mul(_Projection, focalPlane);
  focalPlane /= focalPlane.w;

  //
  // Initialize the rayStart position based on the UV of ray, on the near clip plane.
  // The position is specified in NDC-space, so it doesn't need the camera transform.
  //
  vec4 rayStart = vec4((uv.xy) * 2 - 1, 0, 1);

  //
  // Snap the rayEnd to the _FocusDistance, this is required for the lens sampling / defocus
  // blur to work.
  //
  vec4 rayEnd   = vec4((uv.xy) * 2 - 1, focalPlane.z, 1);

  //
  // Convert rayStart and rayEnd to world-space.
  //
  rayStart = mul(_ProjectionI, rayStart);
  rayStart /= rayStart.w;
  rayStart = mul(_CameraI, rayStart);

  rayEnd = mul(_ProjectionI, rayEnd);
  rayEnd /= rayEnd.w;
  rayEnd = mul(_CameraI, rayEnd);

  //
  // In world-space, setup the ray offset for depth of field (defocus blur).
  //
  const float lensRadius = _Aperture / 2.0;
  const vec3 offset = lensRadius * RandomInUnitDisk(uv);

  //
  // The ray start and end offsets counter each other, which causes any object not at rayEnd depth
  // to become defocused incrementally more the further it is from that distance.
  //
  rayStart.xy += offset.xy;
  rayEnd.xy -= offset.xy;

  //
  // Setup the ray object, converting rayStart/rayEnd to a direction.
  // Color defaults to the sky dome in the direction of the ray and the material is not yet known.
  //
  Ray r;
  r.origin = rayStart.xyz;
  r.direction = rayEnd.xyz - rayStart.xyz;
  r.bounces = 0;
  r.color = vec3(1, 1, 1);
  r.accumColor = vec4(0, 0, 0, 0);
  r.material = kMaterialInvalid;

  return r;
}

// ---------------------------------------------------------------------------------------------- //
// RayTrace Kernel.
//
// Used for primary and secondary rays, the RayTrace() kernel computes one bounce for every ray
// processed. 
// ---------------------------------------------------------------------------------------------- //

[numthreads(8,8,1)]
void RayTrace(uint3 id : SV_DispatchThreadID,
              uint3 groupThreadID : SV_GroupThreadID,
              uint groupIndex : SV_GroupIndex)
{
  uint width;
  uint height;
  _AccumulatedImage.GetDimensions(width, height);

  uint rayCount;
  uint stride;
  _Rays.GetDimensions(rayCount, stride);

  int rayIndex;
  int2 xy;
  vec2 uv;
  Ray r;

  GetNextRay(id, rayCount, width, height, /*outputs*/ xy, uv, rayIndex, r);
  if (length(r.direction) == 0) {
    return;
  }

  //
  // Preconditions: length(r.direction) != 0
  //                rayIndex & xy are initialized for r.
  //

  r.direction = normalize(r.direction);
  bool scattered = Color(r, uv);
    
  if (r.bounces > _MaxBounces || !scattered) {
    // Alpha is actually our sample count, which is used to normalize the accumulated
    // output; by setting alpha=1 here, 1 is added to the sample count for this pixel.
    r.accumColor += vec4(lerp(r.color, float3(0,0,0), r.bounces > _MaxBounces), 1);
    
    r.origin = vec3(0,0,0);
    r.direction = vec3(0,0,0);
    r.color = vec3(0,0,0);
    r.bounces = 0;
  }

  _Rays[rayIndex] = r;
}

// ---------------------------------------------------------------------------------------------- //
// Camera Rays Kernel.
// ---------------------------------------------------------------------------------------------- //

[numthreads(8,8,1)]
void InitCameraRays(uint3 id : SV_DispatchThreadID,
                    uint3 groupThreadID : SV_GroupThreadID,
                    uint groupIndex : SV_GroupIndex)
{
  uint width;
  uint height;
  _AccumulatedImage.GetDimensions(width, height);
  
  int rayIndex = (id.x * height + id.y) + (width * height * id.z);

  if (length(_Rays[rayIndex].direction) > 0) {
    // This ray is being processed.
    return;
  }

  vec2 uv = vec2(id.x / (width - 1.0),
                 id.y / (height - 1.0));

  // Jitter the ray by jittering the UV sample.

  uv += (Noise(uv * 10000 + id.z * 100 + _Time.y) - .5)
      * vec2(1.0 / width, 1.0 / height);

  Ray r = InitCameraRay(uv);
  r.accumColor = _Rays[rayIndex].accumColor;
  _Rays[rayIndex] = r;
}

// ---------------------------------------------------------------------------------------------- //
// Accumulation Normalizing Kernel.
// ---------------------------------------------------------------------------------------------- //

[numthreads(8,8,1)]
void NormalizeSamples(uint3 id : SV_DispatchThreadID)
{
  uint width;
  uint height;
  _AccumulatedImage.GetDimensions(width, height);
  
  //
  // The max() avoids div/0 and min avoids HDR ghosting for very bright values.
  //
  int rayIndex = (id.x * height + id.y) + (width * height * id.z);
  _Rays[rayIndex].accumColor = min(1, 
                                   _Rays[rayIndex].accumColor
                                      / max(1, _Rays[rayIndex].accumColor.a));
  _Rays[rayIndex].accumColor /= 8;
}
